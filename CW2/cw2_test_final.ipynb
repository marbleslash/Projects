{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-umsVQiStUAk"
   },
   "source": [
    "# Test Your Model\n",
    "The code below illustrates how to load the model trained and saved at notebook *cw2-train.ipynb*, and test the performance of the loaded model with held-out test data. \n",
    "\n",
    "**REMARK 1**: You should adjust the code below according to what components you have saved and how you have saved them.\n",
    "\n",
    "**REMARK 2**: When the markers evaluate your model, they will use the code below (but replace the test data with some held-out data) to run the test. **Hence, make sure you can re-load your model and test its performance with the code below.**\n",
    "\n",
    "**REMARK 3**: If you use embeddings to represent text, **DO NOT** include the embeddings file in your submitted file due to its huge size. Instead, specify which pre-trained embedding you want to use or provide a link for downloading it; the markers will download the embedding and run your code below to load the embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "xuvQIslpt30l",
    "outputId": "015a1455-a795-49ad-dc0d-19107ca12031"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "02nI2iDdtUAs"
   },
   "outputs": [],
   "source": [
    "# NOTE! The model defined below MUST BE EXACTLY THE SAME as the one you used at training\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CNN_Clf(nn.Module):\n",
    "    def __init__(self, embd_dim, filter_size_list, filter_num_list, class_num, dp_rate=0.5, gpu=False):\n",
    "        super(CNN_Clf, self).__init__()\n",
    "        self.embd_dim = embd_dim\n",
    "        assert len(filter_size_list) == len(filter_num_list)\n",
    "        self.output_dim = class_num\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(dp_rate)\n",
    "        self.fc = nn.Linear(np.sum(filter_num_list), class_num)\n",
    "        self.gpu = gpu\n",
    "        self.convs = self.build_convs(filter_size_list, filter_num_list, gpu)\n",
    "        if self.gpu:\n",
    "            self.to('cuda')\n",
    "            \n",
    "    def build_convs(self, f_sizes, f_nums, gpu):\n",
    "        convs = nn.ModuleList()\n",
    "        for fs, fn in zip(f_sizes, f_nums):\n",
    "            padding_size = fs-1\n",
    "            m = nn.Conv1d(self.embd_dim, fn, fs, padding=padding_size)\n",
    "            if gpu: m.to('cuda')\n",
    "            convs.append(m)\n",
    "        return convs\n",
    "        \n",
    "    def get_conv_output(self, input_matrix, conv, gpu):\n",
    "        # step 1: compute convolution \n",
    "        assert input_matrix.shape[1] == self.embd_dim\n",
    "        conv_output = conv(input_matrix)\n",
    "        # step 2: pass through an activation function \n",
    "        conv_relu = self.tanh(conv_output)\n",
    "        # step 3: max-over-time pooling\n",
    "        maxp = nn.MaxPool1d(conv_relu.shape[2])\n",
    "        maxp_output = maxp(conv_relu)\n",
    "        return maxp_output\n",
    "       \n",
    "    def forward(self, all_text_vectors):\n",
    "        cnn_repr = torch.tensor([])\n",
    "        if self.gpu: cnn_repr = cnn_repr.to('cuda')\n",
    "        for cv in self.convs:\n",
    "            cv_output = self.get_conv_output(all_text_vectors, cv, self.gpu)\n",
    "            cnn_repr = torch.cat((cnn_repr, cv_output), dim=1)\n",
    "        # print(cnn_repr.shape)\n",
    "        after_dp = self.dropout(cnn_repr.squeeze())\n",
    "        logit = self.fc(after_dp)\n",
    "        # the CrossEntropyLoss provided by pytorch includes softmax; so you do not need to include a softmax layer in your net\n",
    "        return logit\n",
    "#word_vectors['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "l7egH9hytUBD",
    "outputId": "4008ed9f-a458-4854-cf20-80798bd85116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# if you use word embeddings to represent text, provide the script for loading the embeddings below\n",
    "# if you use glove, word2vec or fasttext embeddings, please specify which version you use (e.g. glove.6B.300d)\n",
    "# if you use other embedding models, please provide the download link\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# load pre-trained glove embeddings\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# specify the loaction of the downloaded glove file\n",
    "path_of_downloaded_files = \"/content/drive/My Drive/Colab Notebooks/glove.6B.300d.txt\"\n",
    "glove_file = datapath(path_of_downloaded_files)\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.300d.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "word_vectors = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AhVVSxqDtUBt"
   },
   "outputs": [],
   "source": [
    "# scripts for creating sentence vectors; adjust the code if necessary\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "def vectorize_sent(word_vectors, sent, oov_vec):\n",
    "    word_vecs = []\n",
    "    for token in word_tokenize(sent): \n",
    "        if token not in word_vectors: \n",
    "            word_vecs.append(oov_vec)\n",
    "        else:\n",
    "            word_vecs.append(word_vectors[token].astype('float64'))\n",
    "    return np.mean(word_vecs,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JKjf0lEdtUCm"
   },
   "outputs": [],
   "source": [
    "# reconstruct your trained model from pickle\n",
    "\n",
    "import pickle\n",
    "def reconstruct_model(pickle_path):\n",
    "    saved_model_dic = pickle.load(open(pickle_path,\"rb\"))\n",
    "    input_dim = saved_model_dic['input_dim']\n",
    "    dp_rate = saved_model_dic['dropout_rate']\n",
    "    filter_size_list = saved_model_dic['filter_size_list']\n",
    "    filter_num_list = saved_model_dic['filter_num_list']\n",
    "    output_dim = 2\n",
    "    class_num = saved_model_dic['class_num']\n",
    "    oov_vec = saved_model_dic['oov_vector']\n",
    "    model = CNN_Clf(input_dim, filter_size_list, filter_num_list, class_num)\n",
    "    saved_weights = saved_model_dic['neural_weights']\n",
    "    model.load_state_dict(saved_weights)\n",
    "\n",
    "    \n",
    "    return model, oov_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9V4QK_FttUDd"
   },
   "outputs": [],
   "source": [
    "# use the reconstructed model to make predictions on the test data\n",
    "\n",
    "# use the reconstructed model to make predictions on the test data\n",
    "#oov_vec  = saved_model_dic['oov_vector']\n",
    "def get_sent_word_vecs(word_vectors, sent_words, largest_len, oov_vec):\n",
    "    word_vec_dim = 300\n",
    "    vecs = []\n",
    "    for ww in sent_words:\n",
    "        if ww in word_vectors:\n",
    "            vecs.append(word_vectors[ww])\n",
    "        else:\n",
    "            vecs.append(oov_vec)\n",
    "    for i in range(largest_len-len(sent_words)):\n",
    "        vecs.append([0.]*word_vec_dim)\n",
    "    return np.array(np.transpose(vecs))\n",
    "\n",
    "\n",
    "def build_mini_batch(sent_list, word_vectors, oov_vec):\n",
    "    #oov_vec  = saved_model_dic['oov_vector']\n",
    "    tokenized_sents = [word_tokenize(ss.lower()) for ss in sent_list]\n",
    "    largest_len = np.max([len(tokens) for tokens in tokenized_sents])\n",
    "    text_vecs = []\n",
    "    for ts in tokenized_sents:\n",
    "        vv = get_sent_word_vecs(word_vectors, ts, largest_len, oov_vec)\n",
    "        text_vecs.append(vv)\n",
    "    # print('mini batch shape',np.array(text_vecs).shape)\n",
    "    return np.array(text_vecs)\n",
    "\n",
    "def test_trained_model(model, oov_vec, test_text):\n",
    "    #oov_vec  = saved_model_dic['oov_vector']\n",
    "    test_vecs = build_mini_batch(test_text, word_vectors, oov_vec)\n",
    "    test_vecs_tensor = torch.tensor(test_vecs, dtype=torch.float)\n",
    "    test_prediction = model(test_vecs_tensor)\n",
    "    pred_labels = [np.argmax(tp.detach().numpy()) for tp in test_prediction]\n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JlB1KWRUtUD7",
    "outputId": "14b25359-a0d0-4ef0-e697-9bcec6c5f454"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data size 2000\n",
      "macro-F1 on test data 0.9739348996842183\n"
     ]
    }
   ],
   "source": [
    "# load sample test data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "\n",
    "# df = pd.read_table('/content/drive/My Drive/Colab Notebooks/coursework2_train.tsv')\n",
    "# df = shuffle(df) \n",
    "test_data = pd.read_table('/content/drive/My Drive/Colab Notebooks/coursework2_train.tsv')\n",
    "#test_data = shuffle(test_data)\n",
    "test_text = test_data['sentence_text'].tolist()[-2000:]\n",
    "test_titles = test_data['article_title'].tolist()[-2000:]\n",
    "test_raw_labels = test_data['label'].tolist()[-2000:]\n",
    "label_dic = {'non-propaganda':0, 'propaganda':1} \n",
    "test_labels = [label_dic[rl] for rl in test_raw_labels]\n",
    "\n",
    "print('test data size', len(test_labels))\n",
    "\n",
    "\n",
    "import string\n",
    "\n",
    "combined = []\n",
    "for tits, dics in zip(test_titles, test_text):\n",
    "    combined.append(tits + \" \" + dics)\n",
    "#combined2 = [\" \".join(ss) for ss in combined]\n",
    "\n",
    "combined3 = []\n",
    "for sent in combined:\n",
    "    combined3.append(\" \".join(re.findall('\\w+', sent.lower())))\n",
    "\n",
    "test_input = combined3\n",
    "\n",
    "# reconstruct model and make predictions\n",
    "model, oov_vec = reconstruct_model('/content/drive/My Drive/Colab Notebooks/cw2_sample_saved_file.pickle')\n",
    "test_pred = test_trained_model(model, oov_vec, test_input)\n",
    "\n",
    "# test model\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
    "pre, rec, f1, _ = precision_recall_fscore_support(test_labels, test_pred, average='macro')\n",
    "print('macro-F1 on test data', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "FDDLC3G3wTio",
    "outputId": "3a423e90-b725-4c3f-f2d0-f8464764f3df"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3a4ecd86dbfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcombined3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'combined3' is not defined"
     ]
    }
   ],
   "source": [
    "combined3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "Ep1lpAyZtUEb",
    "outputId": "5ccda8dd-4c65-4811-921b-3bda6fcd3ee8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hinson “told the students that they could do these activities if they wanted,” Branch claimed.',\n",
       " '“The teacher has told her class several times that this is a study of world religions and that she is not trying to advocate for any religion over another.',\n",
       " 'She has told her class that if they had questions about religious beliefs, that those conversations should take place with their parents,” Branch added.',\n",
       " 'Penkoski claimed the teacher sent students home with the same packet the day after he lodged a complaint — this time, with certain sections crossed out but still including the Shahada assignment.',\n",
       " 'Penkoksi called the principal again and confronted Hinson over the phone.',\n",
       " '“I said, ‘This is not OK in asking my kid to write down the Shahada.’ The teacher happened to walk-in and said she made it an option and that the kids didn’t have to do it.',\n",
       " 'My daughter conflicted that story and said, ‘No, that is not what was said.’ What was said was, ‘Do the assignment; and if you want to learn more about the Quran, ask your parents,’” Penkoski told Christian Post.',\n",
       " 'Each religion studied in the class, including Christianity and Judaism, received “equitable treatment,” Branch also told Christian Post.',\n",
       " 'He said the class spent a week and a half on Christianity and Judaism — other religions took one week — and discussed each religion’s beliefs, history and practices.',\n",
       " 'During the section on Christianity, “Jesus was taught,” Branch claimed.']"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[100:110]['sentence_text'].tolist()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cw2-test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
