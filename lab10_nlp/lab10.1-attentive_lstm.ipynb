{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-based Text Classifier with Attentions\n",
    "In this notebook, we will develop an attentive LSTM-based text classifier and check which words the model pays most attention to when it makes predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sentiment analysis data\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "raw_data = pd.read_csv('../class8/coursework1_train.csv') # put the dataset from CW1 under the same directory\n",
    "raw_data = shuffle(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry num 40000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "# check the size of the data and its class distribution\n",
    "\n",
    "# only use a small fraction of the data to speed up training and testing\n",
    "all_text = raw_data['text'].tolist()[:40000] \n",
    "all_raw_labels = raw_data['sentiment'].tolist()[:40000]\n",
    "labels_list = ['pos','neg']\n",
    "all_labels = [labels_list.index(ll) for ll in all_raw_labels]\n",
    "\n",
    "print('entry num', len(all_text))\n",
    "print(len([ll  for ll in  all_labels if ll==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg in train 17496\n",
      "neg in dev 2504\n"
     ]
    }
   ],
   "source": [
    "# data split. \n",
    "# Feel free to use differnt raios to split the data.\n",
    "train_docs = all_text[:35000]\n",
    "train_labels = all_labels[:35000]\n",
    "dev_docs = all_text[35000:]\n",
    "dev_labels = all_labels[35000:]\n",
    "\n",
    "print('neg in train', len([ll for ll in train_labels if ll==1]))\n",
    "print('neg in dev', len([ll for ll in dev_labels if ll==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To be fair, I couldn't bear to watch this movie all the way thru, so I have no idea if it suddenly gets better half way thru the film. But the first 30 minutes or so are amongst the worst I have seen in a while. Children under twelve might get a kick out of the poorly written, acted, and directed slapstick humor, but adults in full control of all their faculties should steer clear of this stinker.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_docs[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained glove embeddings\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# specify the loaction of the downloaded glove file\n",
    "path_of_downloaded_files = \"/home/cim/staff/uhac002/Library/Embeddings/GloVe/glove.6B.300d.txt\"\n",
    "# path_of_downloaded_files = \"/Users/yg211/Embeddings/Glove/glove.6B.300d.txt\"\n",
    "glove_file = datapath(path_of_downloaded_files)\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.300d.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "word_vectors = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we define the attentive RNN-based classifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN_Attention(nn.Module):\n",
    "    def __init__(self, embd_dim, hidden_dim, cls_num, gpu):\n",
    "        super(RNN_Attention, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=embd_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=False)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.fc = nn.Linear(hidden_dim, cls_num)\n",
    "        self.query_vec = torch.randn(1,hidden_dim, requires_grad=True)\n",
    "        self.gpu = gpu\n",
    "        if gpu: \n",
    "            self.to('cuda')\n",
    "            self.query_vec = self.query_vec.to('cuda')\n",
    "    def weighted_sum_vector(self, hidden_states, weights):\n",
    "        # we view the weighted summation of hidden states as a covolutional computation process\n",
    "        # covolutional computation is very efficient to run\n",
    "        aggregation_layer = torch.nn.Conv1d(in_channels=hidden_states.shape[0], out_channels=1, kernel_size=1, bias=False)\n",
    "        if self.gpu: aggregation_layer.to('cuda')\n",
    "        aggregation_layer.weight = torch.nn.Parameter(weights.unsqueeze(2))\n",
    "        # print('weights', aggregation_layer.weight.shape)\n",
    "        weighted_vec = aggregation_layer(hidden_states.unsqueeze(0))\n",
    "        return weighted_vec\n",
    "    def forward(self, input_matrix):\n",
    "        hidden_states = self.lstm(input_matrix.unsqueeze(0))[0].squeeze()\n",
    "        # print('len shape', len(hidden_states.shape))\n",
    "        if len(hidden_states.shape) == 1: hidden_states = hidden_states.unsqueeze(0)\n",
    "        # print('hidden states', hidden_states.shape)\n",
    "        atten_logits = torch.mm(self.query_vec, torch.transpose(hidden_states,0,1))\n",
    "        # print('atten logits', atten_logits.shape)\n",
    "        atten_probs = self.softmax(atten_logits)\n",
    "        # print('--->atten probs', atten_probs)\n",
    "        weighted_vec = self.weighted_sum_vector(hidden_states, atten_probs)\n",
    "        # print('weighted sum vec', weighted_vec.shape)\n",
    "        logits = self.fc(weighted_vec)\n",
    "        return logits, atten_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0302, -0.0265],\n",
      "        [ 0.0210, -0.0037],\n",
      "        [ 0.0429, -0.0247]], device='cuda:0', grad_fn=<ViewBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cim/staff/uhac002/PycharmProjects/ScratchPad/venv_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# define functions that build mini-batches\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "embd_dim = 300\n",
    "hidden_dim = 500\n",
    "gpu = True\n",
    "\n",
    "oov_vec = oov_vec = np.random.rand(embd_dim)\n",
    "\n",
    "def get_sent_word_vecs(word_vectors, sent_words, largest_len):\n",
    "    vecs = []\n",
    "    for ww in sent_words:\n",
    "        if ww in word_vectors:\n",
    "            vecs.append(word_vectors[ww])\n",
    "        else:\n",
    "            vecs.append(oov_vec)\n",
    "    return np.array(vecs)\n",
    "\n",
    "def build_mini_batch(sent_list, word_vectors):\n",
    "    tokenized_sents = [word_tokenize(ss.lower()) for ss in sent_list]\n",
    "    largest_len = np.max([len(tokens) for tokens in tokenized_sents])\n",
    "    text_vecs = []\n",
    "    for ts in tokenized_sents:\n",
    "        vv = get_sent_word_vecs(word_vectors, ts, largest_len)\n",
    "        text_vecs.append(vv)\n",
    "    # print('mini batch shape',np.array(text_vecs).shape)\n",
    "    return np.array(text_vecs)\n",
    "\n",
    "def make_batch_prediction(sent_list, word_vectors, model, use_gpu=False):\n",
    "    batch = build_mini_batch(sent_list, word_vectors)\n",
    "    batch_logits = torch.tensor([])\n",
    "    if use_gpu: batch_logits = batch_logits.to('cuda')\n",
    "    for i in range(batch.shape[0]):\n",
    "        input_sents = torch.from_numpy(batch[i]).float()\n",
    "        if use_gpu: input_sents = input_sents.to('cuda')\n",
    "        logits = model(input_sents)[0]\n",
    "        batch_logits = torch.cat( (batch_logits, logits) )\n",
    "    return batch_logits.view(batch.shape[0],-1)\n",
    "  \n",
    "# sanity check \n",
    "model = RNN_Attention(embd_dim, hidden_dim,len(labels_list), gpu)\n",
    "batch_pred = make_batch_prediction(\n",
    "    ['hello world!','hello','another test sentence this is'],\n",
    "    word_vectors, model, gpu)\n",
    "print(batch_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fnc = torch.nn.CrossEntropyLoss() # cross entropy loss\n",
    "\n",
    "# hyper parameters\n",
    "n_epochs = 20 # number of epoch (i.e. number of iterations)\n",
    "batch_size = 50\n",
    "lr = 0.001 # initial learning rate\n",
    "\n",
    "# init optimizer and scheduler (lr adjustor)\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr) # use Adam as the optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5) # after each epoch, the learning rate is discounted to its 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/home/cim/staff/uhac002/PycharmProjects/ScratchPad/venv_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======epoch 0 loss====== 0.44126064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 1/20 [07:26<2:21:18, 446.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 0 the macro-F1 on dev set is 0.8287997573481847\n",
      "learning rate 0.001\n",
      "best model updated; new best macro-F1 0.8287997573481847\n",
      "\n",
      "======epoch 1 loss====== 0.3628788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [14:55<2:14:11, 447.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 1 the macro-F1 on dev set is 0.8529651292100435\n",
      "learning rate 0.001\n",
      "best model updated; new best macro-F1 0.8529651292100435\n",
      "\n",
      "======epoch 2 loss====== 0.3201975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [22:27<2:07:07, 448.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 2 the macro-F1 on dev set is 0.8607956345510994\n",
      "learning rate 0.001\n",
      "best model updated; new best macro-F1 0.8607956345510994\n",
      "\n",
      "======epoch 3 loss====== 0.28310788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [29:57<1:59:44, 449.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 3 the macro-F1 on dev set is 0.8696510841846157\n",
      "learning rate 0.001\n",
      "best model updated; new best macro-F1 0.8696510841846157\n",
      "\n",
      "======epoch 4 loss====== 0.24828048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [37:27<1:52:19, 449.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 4 the macro-F1 on dev set is 0.8700783465605855\n",
      "learning rate 0.001\n",
      "best model updated; new best macro-F1 0.8700783465605855\n",
      "\n",
      "======epoch 5 loss====== 0.22091138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [44:58<1:44:57, 449.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 5 the macro-F1 on dev set is 0.868544215423257\n",
      "learning rate 0.001\n",
      "\n",
      "======epoch 6 loss====== 0.19731374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [52:30<1:37:35, 450.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 6 the macro-F1 on dev set is 0.86633051325401\n",
      "learning rate 0.001\n",
      "\n",
      "======epoch 7 loss====== 0.18098406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [1:00:00<1:30:03, 450.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 7 the macro-F1 on dev set is 0.8541978946175982\n",
      "learning rate 0.001\n",
      "\n",
      "======epoch 8 loss====== 0.1489057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [1:07:30<1:22:30, 450.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 8 the macro-F1 on dev set is 0.8445177498896916\n",
      "learning rate 0.001\n",
      "\n",
      "======epoch 9 loss====== 0.13356373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [1:14:59<1:14:59, 449.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 9 the macro-F1 on dev set is 0.844150620682657\n",
      "learning rate 0.001\n",
      "\n",
      "======epoch 10 loss====== 0.099182814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [1:22:31<1:07:34, 450.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 10 the macro-F1 on dev set is 0.8487836444389825\n",
      "learning rate 0.0005\n",
      "\n",
      "======epoch 11 loss====== 0.064322025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [1:30:01<1:00:02, 450.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 11 the macro-F1 on dev set is 0.858189511696285\n",
      "learning rate 0.0005\n",
      "\n",
      "======epoch 12 loss====== 0.07073708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [1:37:31<52:31, 450.17s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 12 the macro-F1 on dev set is 0.8395753331032179\n",
      "learning rate 0.0005\n",
      "\n",
      "======epoch 13 loss====== 0.072480164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [1:45:01<45:01, 450.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 13 the macro-F1 on dev set is 0.848998640987769\n",
      "learning rate 0.0005\n",
      "\n",
      "======epoch 14 loss====== 0.049154975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [1:52:33<37:33, 450.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 14 the macro-F1 on dev set is 0.8575868752064191\n",
      "learning rate 0.0005\n",
      "\n",
      "======epoch 15 loss====== 0.044404868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [2:00:03<30:02, 450.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 15 the macro-F1 on dev set is 0.855358198519372\n",
      "learning rate 0.0005\n",
      "\n",
      "======epoch 16 loss====== 0.048141032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [2:07:33<22:30, 450.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 16 the macro-F1 on dev set is 0.8469999938799997\n",
      "learning rate 0.0005\n",
      "\n",
      "======epoch 17 loss====== 0.044136155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [2:15:03<15:00, 450.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 17 the macro-F1 on dev set is 0.8487128586065573\n",
      "learning rate 0.0005\n",
      "\n",
      "======epoch 18 loss====== 0.031458616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [2:22:34<07:30, 450.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 18 the macro-F1 on dev set is 0.8535643634234826\n",
      "learning rate 0.0005\n",
      "\n",
      "======epoch 19 loss====== 0.02578743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [2:30:05<00:00, 450.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 19 the macro-F1 on dev set is 0.8495554924969003\n",
      "learning rate 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training the CNN model\n",
    "\n",
    "best_f1 = -1.\n",
    "best_model = None\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch_i in tqdm(range(n_epochs)):\n",
    "    # the inner loop is over the batches in the dataset\n",
    "    model.train() # let pytorch know that gradients should be computed, so as to update the model\n",
    "    ep_loss = []\n",
    "    for idx in range(0,len(train_docs),batch_size):\n",
    "        # Step 0: Get the data\n",
    "        sents = train_docs[idx:idx+batch_size]\n",
    "        if len(sents) == 0: break\n",
    "        y_target = torch.tensor([train_labels[idx:idx+batch_size]], dtype=torch.int64).squeeze()\n",
    "        if gpu:\n",
    "            y_target = y_target.to('cuda')\n",
    "        \n",
    "        # Step 1: Clear the gradients \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Step 2: Compute the forward pass of the model\n",
    "        y_pred = make_batch_prediction(sents, word_vectors, model, gpu)\n",
    "        pred_labels = [np.argmax(entry) for entry in y_pred.cpu().detach().numpy()]\n",
    "        # print('pred labels', pred_labels)\n",
    "        # print('true labels', y_target)\n",
    "\n",
    "        # Step 3: Compute the loss value that we wish to optimize\n",
    "        loss = loss_fnc(y_pred, y_target)\n",
    "        # print(loss)\n",
    "        ep_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "        # Step 4: Propagate the loss signal backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 4+: clip the gradient, to avoid gradient explosion\n",
    "        nn.utils.clip_grad_value_(model.parameters(), clip_value=3.)\n",
    "\n",
    "        # Step 5: Trigger the optimizer to perform one update\n",
    "        optimizer.step()\n",
    "    \n",
    "    print('\\n======epoch {} loss======'.format(epoch_i),np.mean(ep_loss))\n",
    "    \n",
    "    # after each epoch, we can test the model's performance on the dev set\n",
    "    with torch.no_grad(): # let pytorch know that no gradient should be computed\n",
    "        model.eval() # let the model know that it in test mode, i.e. no gradient and no dropout\n",
    "        predictions = []\n",
    "        test_docs = dev_docs\n",
    "        test_labels = dev_labels\n",
    "        \n",
    "        for idx in range(0,len(test_docs),batch_size):\n",
    "            y_pred = make_batch_prediction(\n",
    "                test_docs[idx:idx+batch_size], word_vectors, model, gpu)\n",
    "            pred_labels = [np.argmax(entry) for entry in y_pred.cpu().detach().numpy()]\n",
    "            predictions += pred_labels\n",
    "        pre, rec, f1, _ = precision_recall_fscore_support(test_labels, predictions,average='macro')\n",
    "        print('\\n---> after epoch {} the macro-F1 on dev set is {}'.format(epoch_i, f1))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('learning rate', param_group['lr'])\n",
    "        \n",
    "        # save the best model\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            print('best model updated; new best macro-F1',f1)\n",
    "    \n",
    "    # (optional) adjust learning rate according to the scheduler\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict_explain(sent, word_vectors, model, use_gpu=False):\n",
    "    batch = build_mini_batch([sent], word_vectors)\n",
    "    input_sent = torch.from_numpy(batch[0]).float()\n",
    "    if use_gpu: input_sent = input_sent.to('cuda')\n",
    "    logits, attentions = model(input_sent)\n",
    "    softmax = nn.Softmax()\n",
    "    probs = softmax(logits.squeeze()).cpu()\n",
    "    print('probability pos: {:3f}, neg: {:3f}'.format(probs[0], probs[1]))\n",
    "    words = word_tokenize(sent)\n",
    "    attentions = list(attentions.squeeze().cpu().detach().numpy())\n",
    "    assert len(words) == len(attentions)\n",
    "    print('attention values', attentions)\n",
    "    plt.bar(words, attentions)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "probability pos: 0.485224, neg: 0.514776\n",
      "attention values [0.86720574, 0.062304128, 0.061120085, 0.009370021]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cim/staff/uhac002/PycharmProjects/ScratchPad/venv_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/cim/staff/uhac002/PycharmProjects/ScratchPad/venv_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAMkUlEQVR4nO3df6zd9V3H8edr7XCYMfaj1z9Gyy5qUZupzNzULcRAMpaULbYxW7QYzJrg+ociJpvGGg1ZukT5kRj/WHVWXUYmygoKuQndajJhKo6tl4BkbdN5U5ht98c6hsRlcVjz9o970MPltveUe9rT++b5SG443+/303vefHN58uV77jmkqpAkrX6vm/QAkqTxMOiS1IRBl6QmDLokNWHQJamJtZN64nXr1tX09PSknl6SVqUnnnji21U1tdSxiQV9enqaubm5ST29JK1KSb5xpmPecpGkJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmJvZO0ZWY3vXwpEeYqGfv+MCkR5B0EfIKXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1MVLQk2xJcjTJfJJdSxy/MskjSZ5M8nSS949/VEnS2Swb9CRrgD3AjcAm4KYkmxYt+31gX1W9C9gO/Mm4B5Uknd0oV+ibgfmqOlZVLwL3AdsWrSngTYPHlwPfHN+IkqRRjBL0K4DjQ9snBvuGfRy4OckJYD/wG0t9oyQ7k8wlmTt16tSrGFeSdCbjelH0JuAzVbUeeD/w2SSv+N5VtbeqZqpqZmpqakxPLUmC0YJ+EtgwtL1+sG/YLcA+gKr6MvAGYN04BpQkjWaUoB8ENia5KsklLLzoObtozb8D7wVI8hMsBN17KpJ0AS0b9Ko6DdwKHACOsPDbLIeS7E6ydbDsY8BHkvwr8DfAjqqq8zW0JOmVRvqfRFfVfhZe7Bzed/vQ48PAteMdTZJ0LnynqCQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaGCnoSbYkOZpkPsmuM6z5xSSHkxxK8tfjHVOStJy1yy1IsgbYA7wPOAEcTDJbVYeH1mwEfhe4tqqeT/JD52tgSdLSRrlC3wzMV9WxqnoRuA/YtmjNR4A9VfU8QFV9a7xjSpKWM0rQrwCOD22fGOwbdjVwdZLHkjyeZMu4BpQkjWbZWy7n8H02AtcD64F/TPKTVfUfw4uS7AR2Alx55ZVjempJEox2hX4S2DC0vX6wb9gJYLaq/ruqngG+zkLgX6aq9lbVTFXNTE1NvdqZJUlLGCXoB4GNSa5KcgmwHZhdtOYhFq7OSbKOhVswx8Y4pyRpGcsGvapOA7cCB4AjwL6qOpRkd5Ktg2UHgOeSHAYeAX67qp47X0NLkl5ppHvoVbUf2L9o3+1Djwv46OBLkjQBvlNUkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqYqSgJ9mS5GiS+SS7zrLug0kqycz4RpQkjWLZoCdZA+wBbgQ2ATcl2bTEusuA3wS+Mu4hJUnLG+UKfTMwX1XHqupF4D5g2xLrPgHcCfzXGOeTJI1olKBfARwf2j4x2Pd/kvwMsKGqHj7bN0qyM8lckrlTp06d87CSpDNb8YuiSV4H/BHwseXWVtXeqpqpqpmpqamVPrUkacgoQT8JbBjaXj/Y95LLgHcCjyZ5Fng3MOsLo5J0YY0S9IPAxiRXJbkE2A7MvnSwql6oqnVVNV1V08DjwNaqmjsvE0uSlrRs0KvqNHArcAA4AuyrqkNJdifZer4HlCSNZu0oi6pqP7B/0b7bz7D2+pWPJUk6V75TVJKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJamKkoCfZkuRokvkku5Y4/tEkh5M8neSLSd4x/lElSWezbNCTrAH2ADcCm4CbkmxatOxJYKaqfgp4ALhr3INKks5ulCv0zcB8VR2rqheB+4Btwwuq6pGq+t5g83Fg/XjHlCQtZ5SgXwEcH9o+Mdh3JrcAn1/qQJKdSeaSzJ06dWr0KSVJyxrri6JJbgZmgLuXOl5Ve6tqpqpmpqamxvnUkvSat3aENSeBDUPb6wf7XibJDcDvAddV1ffHM54kaVSjXKEfBDYmuSrJJcB2YHZ4QZJ3AX8GbK2qb41/TEnScpYNelWdBm4FDgBHgH1VdSjJ7iRbB8vuBt4I3J/kqSSzZ/h2kqTzZJRbLlTVfmD/on23Dz2+YcxzSZLOke8UlaQmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJamJtZMeQBfe9K6HJz3CRD17xwdW9Odf6+cPVn4OdX54hS5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITIwU9yZYkR5PMJ9m1xPEfSPK5wfGvJJke96CSpLNb9sO5kqwB9gDvA04AB5PMVtXhoWW3AM9X1Y8m2Q7cCfzS+RhY0ur3Wv+As/P14WajXKFvBuar6lhVvQjcB2xbtGYbcM/g8QPAe5NkfGNKkpYzysfnXgEcH9o+AfzsmdZU1ekkLwBvA749vCjJTmDnYPO7SY6+mqEvAutY9Pd2IeXOST3z2Hj+Vs5zuDKr+fy940wHLujnoVfVXmDvhXzO8yHJXFXNTHqO1crzt3Kew5Xpev5GueVyEtgwtL1+sG/JNUnWApcDz41jQEnSaEYJ+kFgY5KrklwCbAdmF62ZBT48ePwh4B+qqsY3piRpOcvechncE78VOACsAT5dVYeS7AbmqmoW+Evgs0nmge+wEP3OVv1townz/K2c53BlWp6/eCEtST34TlFJasKgS1ITBn1MkuxI8vZJz6E+kkwn+doS+/8iyaZJzLSaJLktyZEk9056lgvlgv4eenM7gK8B35zwHGquqn510jOsEr8G3FBVJyY9yIXiFfoZDK6OjiT58ySHkvx9kkuTXJPk8SRPJ3kwyVuSfAiYAe5N8lSSSyc9/8UqyUNJnhic053L/4nXvLVJ7h38LD6Q5AeTPJpkBiDJLUm+nuSrg5/VT0564ItBkk8BPwx8PsnvJPlykieT/EuSHxus2ZHk75J8Icm/JblrslOPQVX5tcQXMA2cBq4ZbO8DbgaeBq4b7NsN/PHg8aPAzKTnvti/gLcO/nopC/9F87ZJz3Sxfg1+Bgu4drD9aeC3XvpZA94OPAu8FXg98E/AJyc998XyNTg364A3AWsH+24A/nbweAdwjIU3Qr4B+AawYdJzr+TLWy5n90xVPTV4/ATwI8Cbq+pLg333APdPZLLV67YkvzB4vAHYiO8qPpvjVfXY4PFfAbcNHdsMfKmqvgOQ5H7g6gs832pwOXBPko0s/Avy9UPHvlhVLwAkOczC56Qcf+W3WB285XJ23x96/D/Amyc1SAdJrmfhCuk9VfXTwJMsXBnpzBa/UcQ3jpy7TwCPVNU7gZ/n5T9zi/8ZX9UXuQb93LwAPJ/k5wbbvwK8dLX+n8BlE5lq9bichc/N/16SHwfePemBVoErk7xn8PiXgX8eOnYQuG7wOs5a4IMXfLrV4XL+//OndkxwjvPOoJ+7DwN3J3kauIaF++gAnwE+5YuiZ/UFFl7kOwLcATw+4XlWg6PArw/O2VuAP33pQFWdBP4A+CrwGAv3jF+YwIwXu7uAP0zyJKv8Cnw5vvVfWsWSvLGqvju4Qn+Qhc9aenDSc2kyvEKXVrePJ3mKhd8YegZ4aMLzaIK8QpekJrxCl6QmDLokNWHQJakJgy5JTRh0SWrifwHJX3WkxiiMtAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(gpu)\n",
    "predict_explain(\n",
    "    \"not a big fan\",\n",
    "    word_vectors, model, gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save model and other necessary modules\n",
    "all_info_want_to_save = {\n",
    "    'oov_vec': oov_vec,\n",
    "    'embd_dim': embd_dim,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'cls_num': len(labels_list),\n",
    "    'query_vec': model.query_vec,\n",
    "}\n",
    "save_path = open(\"sample_trained_model.pickle\",\"wb\")\n",
    "pickle.dump(all_info_want_to_save, save_path)\n",
    "save_path.close()\n",
    "\n",
    "torch.save(best_model, 'attentive_rnn_trained_model.state_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
