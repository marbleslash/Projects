{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tunning BERT to train a propaganda detector\n",
    "In this notebook, we will fine tune the pre-trained BERT model to obtain a propaganda detector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>774637726</td>\n",
       "      <td>Liberals Agree – Trump Tougher on Putin than O...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>The other issue that I want to make is that, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9948</th>\n",
       "      <td>762546428</td>\n",
       "      <td>Homeschooling Expands As Parents Seethe Over L...</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>“But when we ask families why do they homescho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9876</th>\n",
       "      <td>705409419</td>\n",
       "      <td>﻿Vatican Theologian Sacked for Questioning “Me...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>One of the world’s most renowned theologians, Fr.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9949</th>\n",
       "      <td>762546428</td>\n",
       "      <td>Homeschooling Expands As Parents Seethe Over L...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>It includes all of that.”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>757843275</td>\n",
       "      <td>EXPERIMENTAL Ebola Vaccine Will Be Administere...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Read more here about Ebola and how it is trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270</th>\n",
       "      <td>701225819</td>\n",
       "      <td>South Florida Muslim Leader Sofian Zakkout’s D...</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>In February 2016, Zakkout circulated on social...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8551</th>\n",
       "      <td>763260610</td>\n",
       "      <td>The Eerie Silence</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>It also reminds us that so-called “meddling” b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4710</th>\n",
       "      <td>7646642839</td>\n",
       "      <td>Foolish Religion Author Gary Wills: ‘The Relig...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>With What the Qur’an Meant: And Why It Matters...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10653</th>\n",
       "      <td>758386255</td>\n",
       "      <td>Pope Francis vs Contemplative Orders</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>The system, in other words, is stacked against...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>769962236</td>\n",
       "      <td>Communist Door Boy Attacks Teen Trump Supporter</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>The attack took place in a Whataburger restaur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11464 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       article_id                                      article_title  \\\n",
       "1620    774637726  Liberals Agree – Trump Tougher on Putin than O...   \n",
       "9948    762546428  Homeschooling Expands As Parents Seethe Over L...   \n",
       "9876    705409419  ﻿Vatican Theologian Sacked for Questioning “Me...   \n",
       "9949    762546428  Homeschooling Expands As Parents Seethe Over L...   \n",
       "941     757843275  EXPERIMENTAL Ebola Vaccine Will Be Administere...   \n",
       "...           ...                                                ...   \n",
       "3270    701225819  South Florida Muslim Leader Sofian Zakkout’s D...   \n",
       "8551    763260610                                  The Eerie Silence   \n",
       "4710   7646642839  Foolish Religion Author Gary Wills: ‘The Relig...   \n",
       "10653   758386255               Pope Francis vs Contemplative Orders   \n",
       "145     769962236    Communist Door Boy Attacks Teen Trump Supporter   \n",
       "\n",
       "                label                                      sentence_text  \n",
       "1620   non-propaganda  The other issue that I want to make is that, y...  \n",
       "9948       propaganda  “But when we ask families why do they homescho...  \n",
       "9876   non-propaganda  One of the world’s most renowned theologians, Fr.  \n",
       "9949   non-propaganda                          It includes all of that.”  \n",
       "941    non-propaganda  Read more here about Ebola and how it is trans...  \n",
       "...               ...                                                ...  \n",
       "3270       propaganda  In February 2016, Zakkout circulated on social...  \n",
       "8551   non-propaganda  It also reminds us that so-called “meddling” b...  \n",
       "4710   non-propaganda  With What the Qur’an Meant: And Why It Matters...  \n",
       "10653  non-propaganda  The system, in other words, is stacked against...  \n",
       "145    non-propaganda  The attack took place in a Whataburger restaur...  \n",
       "\n",
       "[11464 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from CW2 to train and test the model\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "df = pd.read_table('../CW2/coursework2_train.tsv')\n",
    "df = shuffle(df) # randomly shuffle data entries \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data size: 11464, label type num: 2\n",
      "8227 3237\n"
     ]
    }
   ],
   "source": [
    "raw_labels = df.label.values.tolist()\n",
    "docs = df.sentence_text.values.tolist()\n",
    "titles = df.article_title.values.tolist()\n",
    "\n",
    "label_dic = {'non-propaganda':0, 'propaganda':1}\n",
    "\n",
    "assert len(docs) == len(raw_labels) == len(titles)\n",
    "labels = [label_dic[rl] for rl in raw_labels] # transfer raw labels (strings) to integer numbers\n",
    "print('total data size: {}, label type num: {}'.format(len(docs), len(label_dic)))\n",
    "\n",
    "np_num = len([ll for ll in labels if ll == 0])\n",
    "p_num = len([ll for ll in labels if ll == 1])\n",
    "print(np_num, p_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 6781, dev size 2280, test size 2403\n"
     ]
    }
   ],
   "source": [
    "# split the data into train, dev and test\n",
    "import random\n",
    "\n",
    "train_ratio = 0.6\n",
    "dev_ratio = 0.2\n",
    "train_idx = []\n",
    "dev_idx = []\n",
    "test_idx = []\n",
    "for i in range(len(docs)):\n",
    "    rnd = random.random()\n",
    "    if rnd < train_ratio: train_idx.append(i)\n",
    "    elif rnd < train_ratio+dev_ratio: dev_idx.append(i)\n",
    "    else: test_idx.append(i)\n",
    "\n",
    "print('train size {}, dev size {}, test size {}'.format(len(train_idx), len(dev_idx), len(test_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we define the RNN-based classifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "class BERT_Clf(nn.Module):\n",
    "    def __init__(self, cls_num, gpu, bert_type='base'):\n",
    "        super(BERT_Clf, self).__init__()\n",
    "        # check which version of bert is used\n",
    "        if bert_type == 'base':\n",
    "            self.bert_dim = 768 \n",
    "        elif bert_type == 'large':\n",
    "            self.bert_dim = 1024\n",
    "        else:\n",
    "            print('INVALID bert_type!')\n",
    "            return None\n",
    "        # load the tokenizer customized for the bert model\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-{}-uncased'.format(bert_type))\n",
    "        # load the pretrained bert model\n",
    "        self.model = BertModel.from_pretrained('bert-{}-uncased'.format(bert_type))\n",
    "        # map the bert output embeddings to class prediction logits\n",
    "        self.fc = nn.Linear(self.bert_dim, cls_num)\n",
    "        # use gpu or not\n",
    "        self.gpu = gpu\n",
    "        if self.gpu:\n",
    "            self.to('cuda')\n",
    "    def forward(self, input_sents, input_titles=None):\n",
    "        if input_titles is None:\n",
    "            sents = input_sents\n",
    "        else:\n",
    "            assert len(input_titles) == len(input_sents)\n",
    "            sents = [[input_titles[i], input_sents[i]] for i in range(len(input_titles))]\n",
    "        input_batch = self.tokenizer.batch_encode_plus(sents, pad_to_max_length=True, return_tensors='pt')['input_ids']\n",
    "        if self.gpu:\n",
    "            input_batch = input_batch.to('cuda')\n",
    "        sent_reprs = self.model(input_batch)[0][:,0,:]\n",
    "        logits = self.fc(sent_reprs)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the training data is inbalanced, we use simple down-sampling to balance the data\n",
    "# used to train the model\n",
    "import random\n",
    "import numpy as np\n",
    "def down_sample():\n",
    "    np_idx = [i for i in train_idx if labels[i]==0]\n",
    "    p_idx = [i for i in train_idx if labels[i]==1]\n",
    "    each_cat_num = min(len(np_idx), len(p_idx))\n",
    "    random.shuffle(np_idx)\n",
    "    random.shuffle(p_idx)\n",
    "    wanted_idx = np_idx[:each_cat_num] + p_idx[:each_cat_num]\n",
    "    random.shuffle(wanted_idx)\n",
    "    return wanted_idx\n",
    "    \n",
    "wanted_idx = down_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train our BERT-based model, we first check the performance of some simple baseline methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> the macro-F1 of random baseline on dev set is 0.4794524349839533\n"
     ]
    }
   ],
   "source": [
    "# random baseline\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "rand_pred = [random.randint(0,1) for i in range(len(test_idx))]\n",
    "pre, rec, f1, _ = precision_recall_fscore_support(np.array(labels)[test_idx], rand_pred,average='macro')\n",
    "print('\\n---> the macro-F1 of random baseline on dev set is {}'.format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> the macro-F1 of majority baseline on dev set is 0.4140453547915143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cim/staff/uhac002/PycharmProjects/ScratchPad/venv_nlp/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# majority baseline\n",
    "major_pred = [0]*len(test_idx)\n",
    "pre, rec, f1, _ = precision_recall_fscore_support(np.array(labels)[test_idx], major_pred, average='macro')\n",
    "print('\\n---> the macro-F1 of majority baseline on dev set is {}'.format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start to train our BERT-based model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some hyper parameters\n",
    "gpu = True\n",
    "bert_type = 'base'\n",
    "model = BERT_Clf(len(label_dic), gpu, bert_type)\n",
    "use_titles = True\n",
    "\n",
    "loss_fnc = torch.nn.CrossEntropyLoss() # cross entropy loss\n",
    "\n",
    "# hyper parameters\n",
    "n_epochs = 20 # number of epoch (i.e. number of iterations)\n",
    "batch_size = 16\n",
    "lr = 1e-5 # initial learning rate\n",
    "\n",
    "# init optimizer and scheduler (lr adjustor)\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr) # use Adam as the optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5) # after each epoch, the learning rate is discounted to its 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> before training, the macro-F1 on dev set is 0.3916023544800523\n",
      "pred 1 percent 0.6342105263157894\n"
     ]
    }
   ],
   "source": [
    "# before we train the model, we first look at its initial performance on the test set\n",
    "# without performing any training\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "with torch.no_grad(): # let pytorch know that no gradient should be computed\n",
    "    model.eval() # let the model know that it in test mode, i.e. no gradient and no dropout\n",
    "    predictions = []\n",
    "    for i in range(0,len(dev_idx),batch_size):\n",
    "        idx = dev_idx[i:i+batch_size]\n",
    "        if len(idx) == 0: break\n",
    "        dd = np.array(docs)[idx]\n",
    "        if use_titles:\n",
    "            tt = np.array(titles)[idx]\n",
    "        else:\n",
    "            tt = None\n",
    "        y_pred = model(dd,tt).cpu().detach().numpy()\n",
    "        pred_labels = [np.argmax(entry) for entry in y_pred]\n",
    "        predictions += pred_labels\n",
    "    pre, rec, f1, _ = precision_recall_fscore_support(np.array(labels)[dev_idx], predictions,average='macro')\n",
    "    print('\\n---> before training, the macro-F1 on dev set is {}'.format(f1))\n",
    "    print('pred 1 percent', np.sum(predictions)/len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======epoch 0 loss====== 0.62466675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 1/20 [00:52<16:35, 52.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after 0 epochs the macro-F1 on dev set is 0.7070322865988502\n",
      "pred 1 percent 0.3030701754385965\n",
      "learning rate 1e-05\n",
      "best model updated; new best macro-F1 0.7070322865988502\n",
      "\n",
      "======epoch 1 loss====== 0.5305306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [01:45<15:47, 52.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after 1 epochs the macro-F1 on dev set is 0.7433219687435645\n",
      "pred 1 percent 0.3206140350877193\n",
      "learning rate 1e-05\n",
      "best model updated; new best macro-F1 0.7433219687435645\n",
      "\n",
      "======epoch 2 loss====== 0.43168312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [02:39<14:59, 52.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after 2 epochs the macro-F1 on dev set is 0.7202780749202338\n",
      "pred 1 percent 0.4293859649122807\n",
      "learning rate 1e-05\n",
      "\n",
      "======epoch 3 loss====== 0.33732846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [03:32<14:10, 53.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after 3 epochs the macro-F1 on dev set is 0.7435895217736961\n",
      "pred 1 percent 0.38728070175438595\n",
      "learning rate 1e-05\n",
      "best model updated; new best macro-F1 0.7435895217736961\n",
      "\n",
      "======epoch 4 loss====== 0.25101528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [04:26<13:21, 53.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after 4 epochs the macro-F1 on dev set is 0.7355750161245262\n",
      "pred 1 percent 0.39166666666666666\n",
      "learning rate 1e-05\n",
      "\n",
      "======epoch 5 loss====== 0.18710253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [05:20<12:30, 53.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after 5 epochs the macro-F1 on dev set is 0.7441100943984376\n",
      "pred 1 percent 0.34385964912280703\n",
      "learning rate 1e-05\n",
      "best model updated; new best macro-F1 0.7441100943984376\n",
      "\n",
      "======epoch 6 loss====== 0.1405358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [06:15<11:39, 53.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after 6 epochs the macro-F1 on dev set is 0.7441762468347873\n",
      "pred 1 percent 0.3307017543859649\n",
      "learning rate 1e-05\n",
      "best model updated; new best macro-F1 0.7441762468347873\n",
      "\n",
      "======epoch 7 loss====== 0.09768806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [07:09<10:48, 54.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after 7 epochs the macro-F1 on dev set is 0.7496020064880452\n",
      "pred 1 percent 0.2951754385964912\n",
      "learning rate 1e-05\n",
      "best model updated; new best macro-F1 0.7496020064880452\n",
      "\n",
      "======epoch 8 loss====== 0.08561704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [08:04<09:54, 54.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after 8 epochs the macro-F1 on dev set is 0.7537291077792799\n",
      "pred 1 percent 0.2741228070175439\n",
      "learning rate 1e-05\n",
      "best model updated; new best macro-F1 0.7537291077792799\n",
      "\n",
      "======epoch 9 loss====== 0.06689196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [08:58<09:01, 54.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after 9 epochs the macro-F1 on dev set is 0.7557142857142857\n",
      "pred 1 percent 0.3337719298245614\n",
      "learning rate 1e-05\n",
      "best model updated; new best macro-F1 0.7557142857142857\n",
      "\n",
      "======epoch 10 loss====== 0.045698382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [09:52<08:06, 54.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after 10 epochs the macro-F1 on dev set is 0.7587301587301587\n",
      "pred 1 percent 0.2723684210526316\n",
      "learning rate 5e-06\n",
      "best model updated; new best macro-F1 0.7587301587301587\n",
      "\n",
      "======epoch 11 loss====== 0.03257877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [10:46<07:12, 54.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after 11 epochs the macro-F1 on dev set is 0.7493429938539824\n",
      "pred 1 percent 0.2820175438596491\n",
      "learning rate 5e-06\n",
      "\n",
      "======epoch 12 loss====== 0.025948307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [11:39<06:17, 53.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after 12 epochs the macro-F1 on dev set is 0.7651938550178294\n",
      "pred 1 percent 0.3043859649122807\n",
      "learning rate 5e-06\n",
      "best model updated; new best macro-F1 0.7651938550178294\n",
      "\n",
      "======epoch 13 loss====== 0.020871041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [12:34<05:24, 54.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after 13 epochs the macro-F1 on dev set is 0.7594658281642698\n",
      "pred 1 percent 0.30043859649122806\n",
      "learning rate 5e-06\n",
      "\n",
      "======epoch 14 loss====== 0.022603493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [13:27<04:29, 53.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after 14 epochs the macro-F1 on dev set is 0.7506910644057957\n",
      "pred 1 percent 0.25\n",
      "learning rate 5e-06\n",
      "\n",
      "======epoch 15 loss====== 0.014099522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [14:22<03:35, 53.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after 15 epochs the macro-F1 on dev set is 0.7585088568788936\n",
      "pred 1 percent 0.28596491228070176\n",
      "learning rate 5e-06\n",
      "\n",
      "======epoch 16 loss====== 0.02085349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [15:15<02:41, 53.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after 16 epochs the macro-F1 on dev set is 0.7541712714628427\n",
      "pred 1 percent 0.3048245614035088\n",
      "learning rate 5e-06\n",
      "\n",
      "======epoch 17 loss====== 0.016320812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [16:09<01:47, 53.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after 17 epochs the macro-F1 on dev set is 0.7528196250083232\n",
      "pred 1 percent 0.27280701754385966\n",
      "learning rate 5e-06\n",
      "\n",
      "======epoch 18 loss====== 0.0144783575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [17:03<00:54, 54.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after 18 epochs the macro-F1 on dev set is 0.7626583613973783\n",
      "pred 1 percent 0.29605263157894735\n",
      "learning rate 5e-06\n",
      "\n",
      "======epoch 19 loss====== 0.012555533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [17:57<00:00, 53.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after 19 epochs the macro-F1 on dev set is 0.7581762307858491\n",
      "pred 1 percent 0.31491228070175437\n",
      "learning rate 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_f1 = -1.\n",
    "best_model = None\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch_i in tqdm(range(n_epochs)):\n",
    "    # the inner loop is over the batches in the dataset\n",
    "    model.train() # let pytorch know that gradients should be computed, so as to update the model\n",
    "    ep_loss = []\n",
    "    sample_idx = down_sample()\n",
    "    for i in range(0,len(sample_idx),batch_size):\n",
    "        # Step 0: Get the data\n",
    "        idx = sample_idx[i:i+batch_size]\n",
    "        if len(idx)==0: break\n",
    "        sents = list(np.array(docs)[idx])\n",
    "        if use_titles: \n",
    "            tt = list(np.array(titles)[idx])\n",
    "        else:\n",
    "            tt = None\n",
    "        target_labels = list(np.array(labels)[idx])\n",
    "        # print(sents[0])\n",
    "        if len(sents) == 0: break\n",
    "        y_target = torch.tensor(target_labels, dtype=torch.int64).squeeze()\n",
    "        if gpu:\n",
    "            y_target = y_target.to('cuda')\n",
    "        \n",
    "        # Step 1: Clear the gradients \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Step 2: Compute the forward pass of the model\n",
    "        y_pred = model(sents, tt)\n",
    "        # print(y_pred)\n",
    "        yp = y_pred.cpu().detach().numpy()\n",
    "        pred_labels = [np.argmax(entry) for entry in yp]\n",
    "        # print('pred labels', pred_labels)\n",
    "        # print('true labels', y_target)\n",
    "\n",
    "        # Step 3: Compute the loss value that we wish to optimize\n",
    "        loss = loss_fnc(y_pred, y_target)\n",
    "        # print(loss)\n",
    "        ep_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "        # Step 4: Propagate the loss signal backward\n",
    "        loss.backward()\n",
    "\n",
    "        # Step 5: Trigger the optimizer to perform one update\n",
    "        optimizer.step()\n",
    "    \n",
    "    print('\\n======epoch {} loss======'.format(epoch_i),np.mean(ep_loss))\n",
    "    \n",
    "    # after each epoch, we can test the model's performance on the test set\n",
    "    with torch.no_grad(): # let pytorch know that no gradient should be computed\n",
    "        model.eval() # let the model know that it in test mode, i.e. no gradient and no dropout\n",
    "        predictions = []\n",
    "        for i in range(0,len(dev_idx),batch_size):\n",
    "            idx = dev_idx[i:i+batch_size]\n",
    "            if len(idx) == 0: break\n",
    "            dd = np.array(docs)[idx]\n",
    "            if use_titles:\n",
    "                tt = np.array(titles)[idx]\n",
    "            else:\n",
    "                tt = None\n",
    "            y_pred = model(dd,tt).cpu().detach().numpy()\n",
    "            pred_labels = [np.argmax(entry) for entry in y_pred]\n",
    "            predictions += pred_labels\n",
    "        pre, rec, f1, _ = precision_recall_fscore_support(np.array(labels)[dev_idx], predictions,average='macro')\n",
    "        print('\\n---> after {} epochs the macro-F1 on dev set is {}'.format(epoch_i,f1))\n",
    "        print('pred 1 percent', np.sum(predictions)/len(predictions))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('learning rate', param_group['lr'])\n",
    "        \n",
    "        # save the best model\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            print('best model updated; new best macro-F1',f1)\n",
    "    \n",
    "    # (optional) adjust learning rate according to the scheduler\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> the macro-F1 on test set is 0.7560704972259656\n",
      "pred 1 percent 0.3079483978360383\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(best_model)\n",
    "with torch.no_grad(): # let pytorch know that no gradient should be computed\n",
    "    model.eval() # let the model know that it in test mode, i.e. no gradient and no dropout\n",
    "    predictions = []\n",
    "    for i in range(0,len(test_idx),batch_size):\n",
    "        idx = test_idx[i:i+batch_size]\n",
    "        if len(idx) == 0: break\n",
    "        dd = np.array(docs)[idx]\n",
    "        if use_titles:\n",
    "            tt = np.array(titles)[idx]\n",
    "        else:\n",
    "            tt = None\n",
    "        y_pred = model(dd,tt).cpu().detach().numpy()\n",
    "        pred_labels = [np.argmax(entry) for entry in y_pred]\n",
    "        predictions += pred_labels\n",
    "    pre, rec, f1, _ = precision_recall_fscore_support(np.array(labels)[test_idx], predictions,average='macro')\n",
    "    print('\\n---> the macro-F1 on test set is {}'.format(f1))\n",
    "    print('pred 1 percent', np.sum(predictions)/len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to save your trained model, you may uncomment the line below\n",
    "# torch.save(best_model, 'bert_pgd_base_wTitle.state_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "venv_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
