{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project -- Question 1\n",
    "Follow the steps below to clean up the data and extract\n",
    "the basic network as detailed in the lab  handout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-712842205856316&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     28</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     29</span> <span class=\"ansi-red-fg\"># Insert the path pointing to your copy of enron20.seq on HDFS between &#39;&#39;</span>\n",
       "<span class=\"ansi-green-fg\">---&gt; 30</span><span class=\"ansi-red-fg\"> </span>rdd_seq <span class=\"ansi-blue-fg\">=</span> utf8_decode_and_filter<span class=\"ansi-blue-fg\">(</span>sc<span class=\"ansi-blue-fg\">.</span>sequenceFile<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;/Users/ufac001/Work/CS5234/project/enron20.seq&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansi-cyan-fg\">sequenceFile</span><span class=\"ansi-blue-fg\">(self, path, keyClass, valueClass, keyConverter, valueConverter, minSplits, batchSize)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    743</span>         minSplits <span class=\"ansi-blue-fg\">=</span> minSplits <span class=\"ansi-green-fg\">or</span> min<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>defaultParallelism<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    744</span>         jrdd = self._jvm.PythonRDD.sequenceFile(self._jsc, path, keyClass, valueClass,\n",
       "<span class=\"ansi-green-fg\">--&gt; 745</span><span class=\"ansi-red-fg\">                                                 keyConverter, valueConverter, minSplits, batchSize)\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">    746</span>         <span class=\"ansi-green-fg\">return</span> RDD<span class=\"ansi-blue-fg\">(</span>jrdd<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    747</span> \n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n",
       "<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     61</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     65</span>             s <span class=\"ansi-blue-fg\">=</span> e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">.</span>toString<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n",
       "\n",
       "<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.sequenceFile.\n",
       ": org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: /Users/ufac001/Work/CS5234/project/enron20.seq\n",
       "\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n",
       "\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n",
       "\tat org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:45)\n",
       "\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n",
       "\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:212)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:280)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:276)\n",
       "\tat scala.Option.getOrElse(Option.scala:121)\n",
       "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:276)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:280)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:276)\n",
       "\tat scala.Option.getOrElse(Option.scala:121)\n",
       "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:276)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1395)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:392)\n",
       "\tat org.apache.spark.rdd.RDD.take(RDD.scala:1389)\n",
       "\tat org.apache.spark.api.python.SerDeUtil$.pairRDDToPython(SerDeUtil.scala:243)\n",
       "\tat org.apache.spark.api.python.PythonRDD$.sequenceFile(PythonRDD.scala:397)\n",
       "\tat org.apache.spark.api.python.PythonRDD.sequenceFile(PythonRDD.scala)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:295)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from email.parser import Parser\n",
    "from itertools import chain\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Helper function to convert date from string\n",
    "# to a datetime object\n",
    "def date_to_dt(date):\n",
    "    def to_dt(tms):\n",
    "        def tz():\n",
    "            return datetime.timezone(datetime.timedelta(seconds=tms.tm_gmtoff))\n",
    "        return datetime.datetime(tms.tm_year, tms.tm_mon, tms.tm_mday, \n",
    "                      tms.tm_hour, tms.tm_min, tms.tm_sec, \n",
    "                      tzinfo=tz())\n",
    "    return to_dt(time.strptime(date[:-6], '%a, %d %b %Y %H:%M:%S %z'))\n",
    "\n",
    "# Helper function to decode\n",
    "# the emails from the input dataset\n",
    "# and ignore malformated entries \n",
    "def utf8_decode_and_filter(rdd):\n",
    "    def utf_decode(s):\n",
    "        try:\n",
    "          return str(s, 'utf-8')\n",
    "        except:\n",
    "            pass\n",
    "    return rdd.map(lambda x: utf_decode(x[1])).filter(lambda x: x != None)\n",
    "\n",
    "# Insert the path pointing to your copy of enron20.seq on HDFS between ''\n",
    "rdd_seq = utf8_decode_and_filter(sc.sequenceFile('/Users/ufac001/Work/CS5234/project/enron20.seq'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start from `rdd_seq` above as your base RDD. \n",
    "Use `map` to apply `Parser().parsestr` to every record to extract an Email struct that contains\n",
    "a collection of records for every attribute of the Email header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_mail = rdd_seq.map() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply another map transformation `rdd_mail` to create a tuple of the form `(from_email, [list of recipient emails], datetime)`. Use `get()` method on the email struct to extract information\n",
    "stored in the relevant field. For example, if your Email struct element is `x`, then\n",
    "`x.get('From')`, `x.get(To)`, `x.get('Cc')`, `x.get('Bcc')` and `x.get('Date')` will extract the content of the relevant fields. Apply the helper `date_to_dt()` function above to convert the date string to the `datetime` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_full_email_tuples = rdd_mail.map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the `flatMap()` transformation to the above RDD to convert a tuple for each Email into a triple of the form `(sender, recipient, datetime)`. You may find the `val_by_vec` lambda from Assignment 1 useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_email_triples = rdd_full_email_tuples.flatMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply `filter()` transformation(s) as necessary to exclude all self-loops, and leave only the triples where both the sender and the recipient Email addresses are both valid (as per the definition of a valid Email address in Assignment 1) and are ending with `enron.com`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_email_triples_enron = rdd_email_triples.filter().filter()..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply `distinct` to the result to eliminate all duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_triples = rdd_email_triples_enron.distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect and print the resulting RDD."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "CS5234-Spark-Lab7-FinalProjectQ1",
  "notebookId": 712842205856314
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
